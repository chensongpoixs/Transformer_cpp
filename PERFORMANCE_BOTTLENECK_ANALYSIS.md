# CPU ç­‰å¾…æ—¶é—´é•¿ & GPU ä½¿ç”¨æ•ˆç‡ä½ - è¯¦ç»†åˆ†ææŠ¥å‘Š

## ğŸ“‹ é—®é¢˜æ¦‚è¿°

**ç°è±¡ï¼š**
- CPU ç­‰å¾…æ—¶é—´ç‰¹åˆ«é•¿
- GPU ä½¿ç”¨æ•ˆç‡ä¸é«˜ï¼ˆåˆ©ç”¨ç‡ä½ï¼‰
- è®­ç»ƒé€Ÿåº¦æ…¢ï¼ŒGPU ç©ºé—²æ—¶é—´å¤š

**å½±å“ï¼š**
- è®­ç»ƒæ—¶é—´æ˜¾è‘—å¢åŠ 
- GPU èµ„æºæµªè´¹
- æ— æ³•å……åˆ†åˆ©ç”¨ç¡¬ä»¶æ€§èƒ½

---

## ğŸ” ä¸€ã€é—®é¢˜åˆ†ææ¡†æ¶

### 1.1 è®­ç»ƒæµç¨‹æ—¶é—´çº¿åˆ†æ

```
Batch N è®­ç»ƒæµç¨‹ï¼š
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ CPU é˜¶æ®µï¼ˆä¸»çº¿ç¨‹ï¼‰                                           â”‚
â”‚ 1. æ•°æ®åŠ è½½ (collate_fn)          [CPU å¯†é›†å‹]              â”‚
â”‚    - JSON è§£æ                                                 â”‚
â”‚    - SentencePiece åˆ†è¯          [CPU ç“¶é¢ˆ âš ï¸]              â”‚
â”‚    - Batch ç»„è£…                                                 â”‚
â”‚ 2. æ•°æ®ä¼ è¾“ (CPU->GPU)            [å¯èƒ½é˜»å¡ âš ï¸]              â”‚
â”‚ 3. ç­‰å¾… GPU å®Œæˆ                 [CPU ç­‰å¾… âš ï¸âš ï¸]            â”‚
â”‚                                                                 â”‚
â”‚ GPU é˜¶æ®µï¼ˆå¼‚æ­¥æ‰§è¡Œï¼‰                                           â”‚
â”‚ 4. å‰å‘ä¼ æ’­ (forward)             [GPU è®¡ç®—]                 â”‚
â”‚ 5. åå‘ä¼ æ’­ (backward)            [GPU è®¡ç®—]                 â”‚
â”‚ 6. ä¼˜åŒ–å™¨æ›´æ–° (optimizer.step)    [GPU è®¡ç®—]                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 1.2 æ€§èƒ½ç“¶é¢ˆè¯†åˆ«çŸ©é˜µ

| ç“¶é¢ˆç±»å‹ | ç—‡çŠ¶ | å¯èƒ½åŸå›  | ä¼˜å…ˆçº§ |
|---------|------|---------|--------|
| **æ•°æ®åŠ è½½ç“¶é¢ˆ** | CPU ç­‰å¾…æ—¶é—´é•¿ï¼ŒGPU ç©ºé—² | SentencePiece åˆ†è¯æ…¢ã€JSON è§£ææ…¢ | ğŸ”´ é«˜ |
| **åŒæ­¥ç“¶é¢ˆ** | é¢‘ç¹ CPU-GPU åŒæ­¥ | loss.item()ã€é¢‘ç¹ memory_stats | ğŸŸ  ä¸­ |
| **æ•°æ®ä¼ è¾“ç“¶é¢ˆ** | CPU->GPU ä¼ è¾“æ…¢ | æœªä½¿ç”¨ pin_memoryã€éå¼‚æ­¥ä¼ è¾“ | ğŸŸ  ä¸­ |
| **GPU è®¡ç®—ç“¶é¢ˆ** | GPU åˆ©ç”¨ç‡ä½ | batch_size å¤ªå°ã€æ¨¡å‹å¤ªå° | ğŸŸ¡ ä½ |
| **å†…å­˜ç“¶é¢ˆ** | é¢‘ç¹å†…å­˜åˆ†é…/é‡Šæ”¾ | æœªé¢„åˆ†é…ã€é¢‘ç¹ empty_cache | ğŸŸ¡ ä½ |

---

## ğŸ”¬ äºŒã€è¯¦ç»†æ’æŸ¥æ–¹æ³•

### 2.1 é˜¶æ®µ 1ï¼šåŸºç¡€æ€§èƒ½æµ‹é‡

#### æ­¥éª¤ 1.1ï¼šå¯ç”¨è¯¦ç»†æ€§èƒ½æ—¥å¿—

**ç›®æ ‡ï¼š** æµ‹é‡å„ä¸ªé˜¶æ®µçš„è€—æ—¶

**æ–¹æ³•ï¼š**
```cpp
// åœ¨ train.cpp ä¸­ï¼Œæ¯ä¸ª batch è®°å½•è¯¦ç»†æ—¶é—´
auto batch_start = steady_clock::now();

// 1. æ•°æ®åŠ è½½æ—¶é—´
auto collate_start = steady_clock::now();
Batch batch = ...;  // æ•°æ®åŠ è½½
auto collate_end = steady_clock::now();
double collate_time = duration_cast<microseconds>(collate_end - collate_start).count() / 1000.0;

// 2. å‰å‘ä¼ æ’­æ—¶é—´
auto forward_start = steady_clock::now();
out = model->forward(...);
auto forward_end = steady_clock::now();
double forward_time = duration_cast<microseconds>(forward_end - forward_start).count() / 1000.0;

// 3. åå‘ä¼ æ’­æ—¶é—´ï¼ˆåœ¨ loss_compute ä¸­ï¼‰
// 4. æ€» batch æ—¶é—´
auto batch_end = steady_clock::now();
double total_batch_time = duration_cast<microseconds>(batch_end - batch_start).count() / 1000.0;
```

**è¾“å‡ºç¤ºä¾‹ï¼š**
```
Batch 10/100:
  collate_time: 150.5 ms  (å æ¯”: 60%)
  forward_time: 45.2 ms   (å æ¯”: 18%)
  backward_time: 35.8 ms  (å æ¯”: 14%)
  other_time: 19.5 ms     (å æ¯”: 8%)
  total_time: 251.0 ms
```

**åˆ¤æ–­æ ‡å‡†ï¼š**
- å¦‚æœ `collate_time / total_time > 50%` â†’ **æ•°æ®åŠ è½½ç“¶é¢ˆ** ğŸ”´
- å¦‚æœ `forward_time + backward_time < 30%` â†’ **GPU è®¡ç®—ä¸è¶³** ğŸŸ 
- å¦‚æœ `other_time > 20%` â†’ **åŒæ­¥/ç­‰å¾…ç“¶é¢ˆ** ğŸŸ 

---

#### æ­¥éª¤ 1.2ï¼šGPU åˆ©ç”¨ç‡ç›‘æ§

**ç›®æ ‡ï¼š** å®æ—¶ç›‘æ§ GPU ä½¿ç”¨æƒ…å†µ

**æ–¹æ³• 1ï¼šä½¿ç”¨ nvidia-smi**
```bash
# å®æ—¶ç›‘æ§ GPU åˆ©ç”¨ç‡
nvidia-smi -l 1

# æˆ–ä½¿ç”¨ watch
watch -n 1 nvidia-smi
```

**æ–¹æ³• 2ï¼šåœ¨ä»£ç ä¸­é›†æˆ GPU åˆ©ç”¨ç‡æŸ¥è¯¢**

**éœ€è¦æ·»åŠ çš„åŠŸèƒ½ï¼š**
```cpp
// åœ¨ gpu_profiler.h ä¸­æ·»åŠ 
struct GPUUtilization {
    float gpu_utilization;      // GPU åˆ©ç”¨ç‡ (%)
    float memory_utilization;   // æ˜¾å­˜åˆ©ç”¨ç‡ (%)
    float power_usage;          // åŠŸè€— (W)
    int temperature;            // æ¸©åº¦ (Â°C)
};

static GPUUtilization get_gpu_utilization(torch::Device device);
```

**åˆ¤æ–­æ ‡å‡†ï¼š**
- GPU åˆ©ç”¨ç‡ < 30% â†’ **ä¸¥é‡ç“¶é¢ˆ** ğŸ”´
- GPU åˆ©ç”¨ç‡ 30-60% â†’ **ä¸­ç­‰ç“¶é¢ˆ** ğŸŸ 
- GPU åˆ©ç”¨ç‡ > 80% â†’ **æ­£å¸¸** âœ…

---

### 2.2 é˜¶æ®µ 2ï¼šæ•°æ®åŠ è½½ç“¶é¢ˆåˆ†æ

#### æ­¥éª¤ 2.1ï¼šSentencePiece åˆ†è¯æ€§èƒ½åˆ†æ

**é—®é¢˜ï¼š** SentencePiece åˆ†è¯æ˜¯ CPU å¯†é›†å‹æ“ä½œï¼Œå¯èƒ½æˆä¸ºç“¶é¢ˆ

**æ’æŸ¥æ–¹æ³•ï¼š**
```cpp
// åœ¨ tokenizer_wrapper.cpp ä¸­æ·»åŠ æ€§èƒ½æµ‹é‡
auto tokenize_start = steady_clock::now();
auto ids = tokenizer->encode_as_ids(text);
auto tokenize_end = steady_clock::now();
double tokenize_time = duration_cast<microseconds>(tokenize_end - tokenize_start).count() / 1000.0;

// è®°å½•ç»Ÿè®¡ä¿¡æ¯
static std::vector<double> tokenize_times;
tokenize_times.push_back(tokenize_time);
if (tokenize_times.size() % 100 == 0) {
    double avg_time = std::accumulate(tokenize_times.begin(), tokenize_times.end(), 0.0) / tokenize_times.size();
    LOG_INFO("Average tokenization time: " + std::to_string(avg_time) + " ms");
}
```

**ä¼˜åŒ–æ–¹æ¡ˆï¼š**
1. âœ… **å·²å®ç°ï¼šæ‰¹é‡åˆ†è¯** - `encode_as_ids_batch()`
2. âœ… **å·²å®ç°ï¼šå¤šçº¿ç¨‹åˆ†è¯** - ä½¿ç”¨ `std::thread` å¹¶è¡Œå¤„ç†
3. âœ… **å·²å®ç°ï¼šå†…å­˜é¢„åˆ†é…** - å‡å°‘å†…å­˜åˆ†é…å¼€é”€
4. âš ï¸ **å¾…ä¼˜åŒ–ï¼šGPU åŠ é€Ÿåˆ†è¯** - å¦‚æœæ”¯æŒï¼Œä½¿ç”¨ GPU è¿›è¡Œåˆ†è¯

**åˆ¤æ–­æ ‡å‡†ï¼š**
- å•ä¸ªå¥å­åˆ†è¯æ—¶é—´ > 10ms â†’ **éœ€è¦ä¼˜åŒ–** ğŸ”´
- æ‰¹é‡åˆ†è¯ï¼ˆbatch_size=64ï¼‰æ€»æ—¶é—´ > 500ms â†’ **éœ€è¦ä¼˜åŒ–** ğŸ”´

---

#### æ­¥éª¤ 2.2ï¼šæ•°æ®åŠ è½½å™¨æ€§èƒ½åˆ†æ

**æ’æŸ¥æ–¹æ³•ï¼š**
```cpp
// æ£€æŸ¥æ•°æ®åŠ è½½å™¨é…ç½®
LOG_INFO("Data loader configuration:");
LOG_INFO("  workers: " + std::to_string(config.workers));
LOG_INFO("  pin_memory: " + std::string(config.pin_memory ? "true" : "false"));
LOG_INFO("  prefetch_factor: " + std::to_string(config.prefetch_factor));
LOG_INFO("  cache_size: " + std::to_string(config.cache_size));
```

**æ€§èƒ½æµ‹è¯•ï¼š**
```cpp
// æµ‹é‡æ•°æ®åŠ è½½ååé‡
auto loader_start = steady_clock::now();
for (int i = 0; i < 100; ++i) {
    Batch batch = loader->next();
}
auto loader_end = steady_clock::now();
double avg_load_time = duration_cast<microseconds>(loader_end - loader_start).count() / 1000.0 / 100.0;
LOG_INFO("Average batch load time: " + std::to_string(avg_load_time) + " ms");
```

**ä¼˜åŒ–å»ºè®®ï¼š**
- `workers = 0` â†’ **å•çº¿ç¨‹ï¼Œæ€§èƒ½å·®** ğŸ”´ â†’ å»ºè®®è®¾ç½®ä¸º `4-8`
- `pin_memory = false` â†’ **ä¼ è¾“æ…¢** ğŸŸ  â†’ å»ºè®®å¯ç”¨
- `prefetch_factor = 1` â†’ **é¢„å–ä¸è¶³** ğŸŸ  â†’ å»ºè®®è®¾ç½®ä¸º `2-4`
- `cache_size = 0` â†’ **æ—  GPU ç¼“å­˜** ğŸŸ  â†’ å»ºè®®è®¾ç½®ä¸º `2-4`

---

### 2.3 é˜¶æ®µ 3ï¼šCPU-GPU åŒæ­¥ç“¶é¢ˆåˆ†æ

#### æ­¥éª¤ 3.1ï¼šè¯†åˆ«åŒæ­¥ç‚¹

**é—®é¢˜ï¼š** é¢‘ç¹çš„ CPU-GPU åŒæ­¥ä¼šå¯¼è‡´ CPU ç­‰å¾…

**å¸¸è§åŒæ­¥ç‚¹ï¼š**
1. `tensor.item()` - æå–æ ‡é‡å€¼ï¼ˆå¼ºåˆ¶åŒæ­¥ï¼‰
2. `tensor.cpu()` - è½¬ç§»åˆ° CPUï¼ˆå¼ºåˆ¶åŒæ­¥ï¼‰
3. `torch::cuda::synchronize()` - æ˜¾å¼åŒæ­¥
4. `get_memory_stats()` - å†…å­˜ç»Ÿè®¡ï¼ˆå¯èƒ½åŒæ­¥ï¼‰
5. `loss.item()` - æå– loss å€¼ï¼ˆå¼ºåˆ¶åŒæ­¥ï¼‰

**æ’æŸ¥æ–¹æ³•ï¼š**
```cpp
// åœ¨ä»£ç ä¸­æ ‡è®°æ‰€æœ‰åŒæ­¥ç‚¹
auto sync_start = steady_clock::now();
float loss_value = loss_tensor.item<float>();  // âš ï¸ åŒæ­¥ç‚¹
auto sync_end = steady_clock::now();
double sync_time = duration_cast<microseconds>(sync_end - sync_start).count() / 1000.0;

if (sync_time > 1.0) {  // åŒæ­¥æ—¶é—´ > 1ms
    LOG_WARN("Long synchronization detected: " + std::to_string(sync_time) + " ms");
}
```

**å·²å®ç°çš„ä¼˜åŒ–ï¼š**
- âœ… **å»¶è¿Ÿ loss æå–** - æ¯ 10 ä¸ª batch æ‰¹é‡æå–ï¼Œå‡å°‘åŒæ­¥æ¬¡æ•°
- âœ… **å‡å°‘ memory_stats é¢‘ç‡** - æ¯ 50 ä¸ª batch ç»Ÿè®¡ä¸€æ¬¡

**å¾…ä¼˜åŒ–ï¼š**
- âš ï¸ **è¿›ä¸€æ­¥å‡å°‘åŒæ­¥é¢‘ç‡** - è€ƒè™‘æ¯ 20-50 ä¸ª batch æå–ä¸€æ¬¡ loss

---

#### æ­¥éª¤ 3.2ï¼šCUDA Stream åŒæ­¥åˆ†æ

**é—®é¢˜ï¼š** å¦‚æœ CUDA Stream åŒæ­¥ä¸å½“ï¼Œä¼šå¯¼è‡´ CPU ç­‰å¾…

**æ’æŸ¥æ–¹æ³•ï¼š**
```cpp
// æ£€æŸ¥ CUDA Stream ä½¿ç”¨æƒ…å†µ
if (stream_manager) {
    LOG_INFO("CUDA Stream configuration:");
    LOG_INFO("  stream_count: " + std::to_string(stream_manager->num_streams()));
    LOG_INFO("  use_cuda_stream: " + std::string(config.use_cuda_stream ? "true" : "false"));
}

// æµ‹é‡åŒæ­¥æ—¶é—´
auto sync_start = steady_clock::now();
backward_event.synchronize();  // âš ï¸ åŒæ­¥ç‚¹
auto sync_end = steady_clock::now();
double sync_time = duration_cast<microseconds>(sync_end - sync_start).count() / 1000.0;
```

**ä¼˜åŒ–å»ºè®®ï¼š**
- å¦‚æœ `use_cuda_stream = false` â†’ **æœªä½¿ç”¨æµæ°´çº¿å¹¶è¡Œ** ğŸ”´ â†’ å»ºè®®å¯ç”¨
- å¦‚æœåŒæ­¥é¢‘ç‡è¿‡é«˜ï¼ˆæ¯ä¸ª batchï¼‰ â†’ **åŒæ­¥è¿‡å¤š** ğŸŸ  â†’ å·²ä¼˜åŒ–ä¸ºæ¯ 10 ä¸ª batch

---

### 2.4 é˜¶æ®µ 4ï¼šGPU è®¡ç®—æ•ˆç‡åˆ†æ

#### æ­¥éª¤ 4.1ï¼šBatch Size å½±å“åˆ†æ

**é—®é¢˜ï¼š** Batch size å¤ªå°ä¼šå¯¼è‡´ GPU åˆ©ç”¨ç‡ä½

**æ’æŸ¥æ–¹æ³•ï¼š**
```cpp
LOG_INFO("Training configuration:");
LOG_INFO("  batch_size: " + std::to_string(config.batch_size));
LOG_INFO("  d_model: " + std::to_string(config.d_model));
LOG_INFO("  n_layers: " + std::to_string(config.n_layers));

// è®¡ç®—ç†è®º GPU åˆ©ç”¨ç‡
// GPU åˆ©ç”¨ç‡ â‰ˆ (forward_time + backward_time) / total_batch_time
double gpu_utilization = (forward_time + backward_time) / total_batch_time * 100.0;
LOG_INFO("Estimated GPU utilization: " + std::to_string(gpu_utilization) + "%");
```

**ä¼˜åŒ–å»ºè®®ï¼š**
- Batch size < 16 â†’ **å¤ªå°** ğŸ”´ â†’ å»ºè®®å¢åŠ åˆ° 32-64
- Batch size 16-32 â†’ **ä¸­ç­‰** ğŸŸ  â†’ å¯ä»¥å°è¯•å¢åŠ åˆ° 64-128
- Batch size > 64 â†’ **æ­£å¸¸** âœ…

---

#### æ­¥éª¤ 4.2ï¼šæ¨¡å‹å¤§å°å½±å“åˆ†æ

**é—®é¢˜ï¼š** æ¨¡å‹å¤ªå°ï¼ŒGPU è®¡ç®—æ—¶é—´çŸ­ï¼Œæ— æ³•å……åˆ†åˆ©ç”¨ GPU

**æ’æŸ¥æ–¹æ³•ï¼š**
```cpp
// è®¡ç®—æ¨¡å‹å‚æ•°é‡
size_t total_params = 0;
for (const auto& param : model->parameters()) {
    total_params += param.numel();
}
LOG_INFO("Model parameters: " + std::to_string(total_params / 1000000) + "M");

// æµ‹é‡å‰å‘ä¼ æ’­æ—¶é—´
auto forward_start = steady_clock::now();
out = model->forward(...);
auto forward_end = steady_clock::now();
double forward_time = duration_cast<microseconds>(forward_end - forward_start).count() / 1000.0;
```

**åˆ¤æ–­æ ‡å‡†ï¼š**
- å‰å‘ä¼ æ’­æ—¶é—´ < 10ms â†’ **æ¨¡å‹å¤ªå°æˆ– batch å¤ªå°** ğŸŸ 
- å‰å‘ä¼ æ’­æ—¶é—´ 10-50ms â†’ **æ­£å¸¸** âœ…
- å‰å‘ä¼ æ’­æ—¶é—´ > 100ms â†’ **æ¨¡å‹å¤ªå¤§æˆ– batch å¤ªå¤§** ğŸŸ¡

---

### 2.5 é˜¶æ®µ 5ï¼šæ•°æ®ä¼ è¾“ç“¶é¢ˆåˆ†æ

#### æ­¥éª¤ 5.1ï¼šCPU->GPU ä¼ è¾“æ€§èƒ½

**æ’æŸ¥æ–¹æ³•ï¼š**
```cpp
// æµ‹é‡æ•°æ®ä¼ è¾“æ—¶é—´
auto transfer_start = steady_clock::now();
batch.src = batch.src.to(device, true);  // non_blocking=true
batch.trg = batch.trg.to(device, true);
// ... å…¶ä»–å¼ é‡
auto transfer_end = steady_clock::now();
double transfer_time = duration_cast<microseconds>(transfer_end - transfer_start).count() / 1000.0;

// è®¡ç®—ä¼ è¾“å¸¦å®½
size_t data_size = batch.src.numel() * sizeof(float) + 
                   batch.trg.numel() * sizeof(float) + 
                   ...;  // å…¶ä»–å¼ é‡
double bandwidth = data_size / 1024.0 / 1024.0 / (transfer_time / 1000.0);  // MB/s
LOG_INFO("Data transfer bandwidth: " + std::to_string(bandwidth) + " MB/s");
```

**åˆ¤æ–­æ ‡å‡†ï¼š**
- ä¼ è¾“å¸¦å®½ < 5 GB/s â†’ **pin_memory æœªå¯ç”¨æˆ–ä¼ è¾“æ…¢** ğŸ”´
- ä¼ è¾“å¸¦å®½ 5-10 GB/s â†’ **æ­£å¸¸** âœ…
- ä¼ è¾“å¸¦å®½ > 10 GB/s â†’ **ä¼˜ç§€** âœ…

**ä¼˜åŒ–å»ºè®®ï¼š**
- âœ… **å·²å®ç°ï¼špin_memory** - å¯ç”¨å›ºå®šå†…å­˜
- âœ… **å·²å®ç°ï¼šnon_blocking=true** - å¼‚æ­¥ä¼ è¾“
- âš ï¸ **å¾…ä¼˜åŒ–ï¼šé¢„å–æ›´å¤š batch** - å¢åŠ  cache_size

---

## ğŸ› ï¸ ä¸‰ã€ç³»ç»ŸåŒ–æ’æŸ¥æµç¨‹

### 3.1 å¿«é€Ÿè¯Šæ–­è„šæœ¬

**åˆ›å»ºæ€§èƒ½è¯Šæ–­å‡½æ•°ï¼š**

```cpp
// åœ¨ train.cpp ä¸­æ·»åŠ 
void diagnose_performance_bottleneck(
    torch::Device device,
    const TransformerConfig& config,
    double collate_time_ms,
    double forward_time_ms,
    double backward_time_ms,
    double total_batch_time_ms) {
    
    LOG_INFO("=== Performance Bottleneck Diagnosis ===");
    
    // 1. è®¡ç®—å„é˜¶æ®µå æ¯”
    double collate_ratio = collate_time_ms / total_batch_time_ms * 100.0;
    double compute_ratio = (forward_time_ms + backward_time_ms) / total_batch_time_ms * 100.0;
    double other_ratio = 100.0 - collate_ratio - compute_ratio;
    
    LOG_INFO("Time distribution:");
    LOG_INFO("  Data loading (collate): " + std::to_string(collate_ratio) + "%");
    LOG_INFO("  GPU computation: " + std::to_string(compute_ratio) + "%");
    LOG_INFO("  Other (sync/wait): " + std::to_string(other_ratio) + "%");
    
    // 2. è¯†åˆ«ç“¶é¢ˆ
    if (collate_ratio > 50.0) {
        LOG_WARN("ğŸ”´ BOTTLENECK: Data loading is the bottleneck!");
        LOG_INFO("  Recommendations:");
        LOG_INFO("    1. Increase --workers (current: " + std::to_string(config.workers) + ")");
        LOG_INFO("    2. Enable data cache: --cache-size 2");
        LOG_INFO("    3. Optimize tokenization (batch processing)");
    }
    
    if (compute_ratio < 30.0) {
        LOG_WARN("ğŸ”´ BOTTLENECK: GPU computation time is too low!");
        LOG_INFO("  Recommendations:");
        LOG_INFO("    1. Increase --batch-size (current: " + std::to_string(config.batch_size) + ")");
        LOG_INFO("    2. Check if model is too small");
    }
    
    if (other_ratio > 20.0) {
        LOG_WARN("ğŸŸ  WARNING: High synchronization/wait time!");
        LOG_INFO("  Recommendations:");
        LOG_INFO("    1. Enable --use-cuda-stream true");
        LOG_INFO("    2. Reduce loss extraction frequency");
    }
    
    // 3. GPU åˆ©ç”¨ç‡ä¼°ç®—
    double estimated_gpu_util = compute_ratio;
    if (estimated_gpu_util < 30.0) {
        LOG_WARN("ğŸ”´ GPU utilization is very low: " + std::to_string(estimated_gpu_util) + "%");
    } else if (estimated_gpu_util < 60.0) {
        LOG_WARN("ğŸŸ  GPU utilization is moderate: " + std::to_string(estimated_gpu_util) + "%");
    } else {
        LOG_INFO("âœ… GPU utilization is good: " + std::to_string(estimated_gpu_util) + "%");
    }
    
    LOG_INFO("========================================");
}
```

---

### 3.2 è¯¦ç»†æ€§èƒ½åˆ†æå·¥å…·

**åœ¨è®­ç»ƒå¾ªç¯ä¸­é›†æˆï¼š**

```cpp
// åœ¨ run_epoch ä¸­ï¼Œæ¯ N ä¸ª batch è¾“å‡ºè¯¦ç»†åˆ†æ
if (i % 50 == 0 && i > 0) {
    // è®¡ç®—å¹³å‡æ—¶é—´
    double avg_collate = collate_time_sum / 50.0;
    double avg_forward = forward_time_sum / 50.0;
    double avg_backward = backward_time_sum / 50.0;
    double avg_total = total_time_sum / 50.0;
    
    // è¯Šæ–­ç“¶é¢ˆ
    diagnose_performance_bottleneck(device, config, 
                                   avg_collate, avg_forward, avg_backward, avg_total);
    
    // é‡ç½®è®¡æ•°å™¨
    collate_time_sum = 0.0;
    forward_time_sum = 0.0;
    backward_time_sum = 0.0;
    total_time_sum = 0.0;
}
```

---

## ğŸ“Š å››ã€å¸¸è§é—®é¢˜ä¸è§£å†³æ–¹æ¡ˆ

### 4.1 é—®é¢˜ 1ï¼šæ•°æ®åŠ è½½æ˜¯ç“¶é¢ˆï¼ˆcollate_time > 50%ï¼‰

**ç—‡çŠ¶ï¼š**
- CPU ç­‰å¾…æ—¶é—´é•¿
- GPU ç©ºé—²æ—¶é—´å¤š
- æ•°æ®åŠ è½½æ—¶é—´å æ¯” > 50%

**åŸå› åˆ†æï¼š**
1. SentencePiece åˆ†è¯æ…¢ï¼ˆCPU å¯†é›†å‹ï¼‰
2. å•çº¿ç¨‹æ•°æ®åŠ è½½
3. æœªä½¿ç”¨æ•°æ®ç¼“å­˜

**è§£å†³æ–¹æ¡ˆï¼š**

**æ–¹æ¡ˆ Aï¼šå¯ç”¨å¤šè¿›ç¨‹æ•°æ®åŠ è½½**
```bash
transformer.exe --data ./data --workers 8 --pin-memory true --prefetch-factor 4
```

**æ–¹æ¡ˆ Bï¼šå¯ç”¨ GPU æ•°æ®ç¼“å­˜**
```bash
transformer.exe --data ./data --cache-size 4
```

**æ–¹æ¡ˆ Cï¼šä¼˜åŒ–åˆ†è¯æ€§èƒ½**
- âœ… å·²å®ç°ï¼šæ‰¹é‡åˆ†è¯
- âœ… å·²å®ç°ï¼šå¤šçº¿ç¨‹åˆ†è¯
- âš ï¸ å¾…ä¼˜åŒ–ï¼šè€ƒè™‘ä½¿ç”¨ GPU åŠ é€Ÿåˆ†è¯ï¼ˆå¦‚æœæ”¯æŒï¼‰

---

### 4.2 é—®é¢˜ 2ï¼šGPU åˆ©ç”¨ç‡ä½ï¼ˆ< 30%ï¼‰

**ç—‡çŠ¶ï¼š**
- GPU åˆ©ç”¨ç‡ < 30%
- å‰å‘+åå‘æ—¶é—´å æ¯” < 30%

**åŸå› åˆ†æï¼š**
1. Batch size å¤ªå°
2. æ¨¡å‹å¤ªå°
3. æ•°æ®åŠ è½½å¤ªæ…¢ï¼ˆGPU åœ¨ç­‰å¾…æ•°æ®ï¼‰

**è§£å†³æ–¹æ¡ˆï¼š**

**æ–¹æ¡ˆ Aï¼šå¢åŠ  Batch Size**
```bash
transformer.exe --data ./data --batch-size 128
```

**æ–¹æ¡ˆ Bï¼šå¯ç”¨ CUDA Stream**
```bash
transformer.exe --data ./data --use-cuda-stream true --cuda-stream-count 4
```

**æ–¹æ¡ˆ Cï¼šä¼˜åŒ–æ•°æ®åŠ è½½ï¼ˆè§é—®é¢˜ 1ï¼‰**

---

### 4.3 é—®é¢˜ 3ï¼šCPU-GPU åŒæ­¥è¿‡å¤š

**ç—‡çŠ¶ï¼š**
- åŒæ­¥æ—¶é—´å æ¯” > 20%
- é¢‘ç¹çš„ `loss.item()` è°ƒç”¨

**åŸå› åˆ†æï¼š**
1. æ¯ä¸ª batch éƒ½æå– loss å€¼
2. é¢‘ç¹çš„å†…å­˜ç»Ÿè®¡
3. è¿‡å¤šçš„ Event åŒæ­¥

**è§£å†³æ–¹æ¡ˆï¼š**

**æ–¹æ¡ˆ Aï¼šå‡å°‘ Loss æå–é¢‘ç‡**
- âœ… å·²å®ç°ï¼šæ¯ 10 ä¸ª batch æå–ä¸€æ¬¡

**æ–¹æ¡ˆ Bï¼šå‡å°‘å†…å­˜ç»Ÿè®¡é¢‘ç‡**
- âœ… å·²å®ç°ï¼šæ¯ 50 ä¸ª batch ç»Ÿè®¡ä¸€æ¬¡

**æ–¹æ¡ˆ Cï¼šä½¿ç”¨éé˜»å¡åŒæ­¥**
- âœ… å·²å®ç°ï¼šä½¿ç”¨ Event query è€Œé synchronize

---

### 4.4 é—®é¢˜ 4ï¼šæ•°æ®ä¼ è¾“æ…¢

**ç—‡çŠ¶ï¼š**
- CPU->GPU ä¼ è¾“æ—¶é—´ > 50ms
- ä¼ è¾“å¸¦å®½ < 5 GB/s

**åŸå› åˆ†æï¼š**
1. æœªå¯ç”¨ pin_memory
2. æœªä½¿ç”¨å¼‚æ­¥ä¼ è¾“
3. æ•°æ®ä¼ è¾“æœªæµæ°´çº¿åŒ–

**è§£å†³æ–¹æ¡ˆï¼š**

**æ–¹æ¡ˆ Aï¼šå¯ç”¨ pin_memory**
```bash
transformer.exe --data ./data --pin-memory true
```

**æ–¹æ¡ˆ Bï¼šä½¿ç”¨æ•°æ®ç¼“å­˜**
```bash
transformer.exe --data ./data --cache-size 2
```

**æ–¹æ¡ˆ Cï¼šä½¿ç”¨ CUDA Stream æµæ°´çº¿**
```bash
transformer.exe --data ./data --use-cuda-stream true
```

---

## ğŸ”§ äº”ã€ä¼˜åŒ–æ£€æŸ¥æ¸…å•

### 5.1 æ•°æ®åŠ è½½ä¼˜åŒ–

- [ ] `--workers` è®¾ç½®ä¸º 4-8ï¼ˆå¤šè¿›ç¨‹åŠ è½½ï¼‰
- [ ] `--pin-memory true`ï¼ˆå¯ç”¨å›ºå®šå†…å­˜ï¼‰
- [ ] `--prefetch-factor 2-4`ï¼ˆå¢åŠ é¢„å–ï¼‰
- [ ] `--cache-size 2-4`ï¼ˆGPU æ•°æ®ç¼“å­˜ï¼‰
- [ ] æ‰¹é‡åˆ†è¯å·²å¯ç”¨
- [ ] å¤šçº¿ç¨‹åˆ†è¯å·²å¯ç”¨

### 5.2 GPU è®¡ç®—ä¼˜åŒ–

- [ ] `--batch-size` è‡³å°‘ 32ï¼ˆå»ºè®® 64-128ï¼‰
- [ ] `--use-cuda-stream true`ï¼ˆå¯ç”¨ CUDA Streamï¼‰
- [ ] `--cuda-stream-count 4`ï¼ˆä½¿ç”¨ 4 ä¸ª Streamï¼‰
- [ ] `--use-amp true`ï¼ˆæ··åˆç²¾åº¦è®­ç»ƒï¼Œå¦‚æœæ”¯æŒï¼‰

### 5.3 åŒæ­¥ä¼˜åŒ–

- [ ] å»¶è¿Ÿ loss æå–ï¼ˆæ¯ 10 ä¸ª batchï¼‰
- [ ] å‡å°‘å†…å­˜ç»Ÿè®¡é¢‘ç‡ï¼ˆæ¯ 50 ä¸ª batchï¼‰
- [ ] ä½¿ç”¨ Event éé˜»å¡æŸ¥è¯¢
- [ ] å‡å°‘ä¸å¿…è¦çš„ CPU-GPU åŒæ­¥

### 5.4 å†…å­˜ä¼˜åŒ–

- [ ] åŠæ—¶é‡Šæ”¾å¼ é‡å¼•ç”¨
- [ ] ä½¿ç”¨ empty_cache() æ¸…ç†ç¼“å­˜ï¼ˆé€‚åº¦ï¼‰
- [ ] é¿å…é¢‘ç¹çš„å†…å­˜åˆ†é…/é‡Šæ”¾

---

## ğŸ“ˆ å…­ã€æ€§èƒ½åŸºå‡†æµ‹è¯•

### 6.1 ç†æƒ³æ€§èƒ½æŒ‡æ ‡

**ç›®æ ‡æ€§èƒ½ï¼ˆå‚è€ƒå€¼ï¼‰ï¼š**
- GPU åˆ©ç”¨ç‡ï¼š> 80%
- æ•°æ®åŠ è½½æ—¶é—´å æ¯”ï¼š< 30%
- GPU è®¡ç®—æ—¶é—´å æ¯”ï¼š> 50%
- åŒæ­¥æ—¶é—´å æ¯”ï¼š< 10%
- Batch ååé‡ï¼š> 10 samples/sï¼ˆå–å†³äºæ¨¡å‹å¤§å°ï¼‰

### 6.2 æ€§èƒ½æµ‹è¯•å‘½ä»¤

```bash
# æµ‹è¯• 1ï¼šåŸºç¡€é…ç½®
transformer.exe --data ./data --batch-size 32 --workers 0

# æµ‹è¯• 2ï¼šä¼˜åŒ–é…ç½®
transformer.exe --data ./data --batch-size 64 --workers 8 --pin-memory true --prefetch-factor 4 --cache-size 2 --use-cuda-stream true

# æµ‹è¯• 3ï¼šæè‡´ä¼˜åŒ–
transformer.exe --data ./data --batch-size 128 --workers 8 --pin-memory true --prefetch-factor 4 --cache-size 4 --use-cuda-stream true --cuda-stream-count 4
```

### 6.3 æ€§èƒ½å¯¹æ¯”è¡¨

| é…ç½® | GPU åˆ©ç”¨ç‡ | æ•°æ®åŠ è½½å æ¯” | è®­ç»ƒé€Ÿåº¦ | æ¨èåº¦ |
|------|-----------|------------|---------|--------|
| åŸºç¡€ï¼ˆworkers=0ï¼‰ | 20-30% | 60-70% | æ…¢ | âŒ |
| ä¼˜åŒ–ï¼ˆworkers=8ï¼‰ | 40-60% | 40-50% | ä¸­ç­‰ | ğŸŸ  |
| æè‡´ï¼ˆ+cache+streamï¼‰ | 70-90% | 20-30% | å¿« | âœ… |

---

## ğŸ¯ ä¸ƒã€å¿«é€Ÿè¯Šæ–­å‘½ä»¤

### 7.1 ä¸€é”®è¯Šæ–­

```bash
# è¿è¡Œè®­ç»ƒå¹¶è¾“å‡ºè¯¦ç»†æ€§èƒ½åˆ†æ
transformer.exe --data ./data --batch-size 64 --workers 8 --pin-memory true
```

**æŸ¥çœ‹æ—¥å¿—è¾“å‡ºï¼š**
- æŸ¥æ‰¾ "Performance Bottleneck Diagnosis" éƒ¨åˆ†
- æŸ¥çœ‹å„é˜¶æ®µæ—¶é—´å æ¯”
- æ ¹æ®å»ºè®®è°ƒæ•´å‚æ•°

### 7.2 å®æ—¶ç›‘æ§

```bash
# ç»ˆç«¯ 1ï¼šè¿è¡Œè®­ç»ƒ
transformer.exe --data ./data ...

# ç»ˆç«¯ 2ï¼šç›‘æ§ GPU
watch -n 1 nvidia-smi

# ç»ˆç«¯ 3ï¼šç›‘æ§ CPU
top -p $(pgrep transformer)
```

---

## ğŸ“ å…«ã€æ€»ç»“ä¸å»ºè®®

### 8.1 ä¼˜å…ˆçº§æ’åº

1. **ğŸ”´ é«˜ä¼˜å…ˆçº§ï¼šæ•°æ®åŠ è½½ä¼˜åŒ–**
   - å¯ç”¨å¤šè¿›ç¨‹åŠ è½½ï¼ˆ`--workers 8`ï¼‰
   - å¯ç”¨æ•°æ®ç¼“å­˜ï¼ˆ`--cache-size 2`ï¼‰
   - ä¼˜åŒ–åˆ†è¯æ€§èƒ½ï¼ˆå·²å®ç°æ‰¹é‡+å¤šçº¿ç¨‹ï¼‰

2. **ğŸŸ  ä¸­ä¼˜å…ˆçº§ï¼šGPU è®¡ç®—ä¼˜åŒ–**
   - å¢åŠ  batch sizeï¼ˆ`--batch-size 64-128`ï¼‰
   - å¯ç”¨ CUDA Streamï¼ˆ`--use-cuda-stream true`ï¼‰

3. **ğŸŸ¡ ä½ä¼˜å…ˆçº§ï¼šåŒæ­¥ä¼˜åŒ–**
   - å‡å°‘åŒæ­¥é¢‘ç‡ï¼ˆå·²å®ç°ï¼‰
   - ä½¿ç”¨éé˜»å¡åŒæ­¥ï¼ˆå·²å®ç°ï¼‰

### 8.2 æ¨èé…ç½®

**æ ‡å‡†è®­ç»ƒé…ç½®ï¼š**
```bash
transformer.exe \
  --data ./data \
  --batch-size 64 \
  --workers 8 \
  --pin-memory true \
  --prefetch-factor 4 \
  --cache-size 2 \
  --use-cuda-stream true \
  --cuda-stream-count 4
```

**é«˜æ€§èƒ½è®­ç»ƒé…ç½®ï¼š**
```bash
transformer.exe \
  --data ./data \
  --batch-size 128 \
  --workers 8 \
  --pin-memory true \
  --prefetch-factor 4 \
  --cache-size 4 \
  --use-cuda-stream true \
  --cuda-stream-count 4 \
  --use-amp true
```

---

## ğŸ”— ä¹ã€ç›¸å…³æ–‡æ¡£

- `GPU_EFFICIENCY_ANALYSIS.md` - GPU æ•ˆç‡åˆ†æ
- `CUDA_STREAM_ANALYSIS.md` - CUDA Stream åˆ†æ
- `BATCH_TOKENIZATION_ANALYSIS.md` - æ‰¹é‡åˆ†è¯åˆ†æ
- `MULTI_PROCESS_LOADER_README.md` - å¤šè¿›ç¨‹åŠ è½½å™¨æ–‡æ¡£

---

**æœ€åæ›´æ–°ï¼š** 2026-01-01
**ç‰ˆæœ¬ï¼š** 1.0

