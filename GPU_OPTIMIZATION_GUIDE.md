# GPU性能优化指南

## 一、排查GPU使用不足的方法

### 1. 使用内置性能分析工具

训练程序会在第一个epoch结束后自动输出性能分析报告：

```
========== GPU性能分析 ==========
操作名称 | 总时间(ms) | 次数 | 平均(ms) | 最小(ms) | 最大(ms) | 占比(%)
-----------------------------------
collate_fn          |    1500.00 |     5 |  300.00 |  280.00 |  320.00 |  60.0
forward             |     800.00 |     5 |  160.00 |  150.00 |  170.00 |  32.0
loss_compute        |     200.00 |     5 |   40.00 |   35.00 |   45.00 |   8.0
```

**分析要点：**
- 如果 `collate_fn` 占比 > 50%，说明数据预处理是瓶颈
- 如果 `forward` 占比 < 20%，说明GPU计算时间短，可能batch太小
- 如果各操作时间差异大，说明存在同步等待

### 2. 使用 nvidia-smi 监控

在另一个终端运行：
```bash
# Windows PowerShell
nvidia-smi -l 1

# Linux
watch -n 1 nvidia-smi
```

**关键指标：**
- **GPU利用率 (Util%)**：应该 > 80%，如果 < 50% 说明使用不足
- **显存使用 (Memory-Usage)**：应该 > 50%，如果 < 20% 说明batch太小
- **功耗 (Power)**：应该接近TDP，如果很低说明GPU空闲

### 3. 检查程序输出的GPU状态

训练开始时会自动输出：
```
========== GPU状态检查 ==========
GPU数量: 1
当前使用GPU: 0
GPU名称: NVIDIA GeForce RTX 3090
显存大小: 24.00 GB
已分配显存: 2048.00 MB
缓存显存: 512.00 MB
总使用显存: 2560.00 MB
```

## 二、常见问题及解决方案

### 问题1：GPU利用率低（< 50%）

**可能原因：**
1. Batch size太小
2. 数据预处理在CPU上阻塞GPU
3. 模型太小，无法充分利用GPU

**解决方案：**

#### 方案1：增加Batch Size
```bash
# 逐步增加batch size，直到显存接近满载
transformer.exe --data ./data --batch-size 64
transformer.exe --data ./data --batch-size 128
transformer.exe --data ./data --batch-size 256
```

**判断标准：**
- 显存使用率 > 80%
- GPU利用率 > 80%
- 训练速度提升

#### 方案2：优化数据预处理（未来实现）
当前 `collate_fn` 在CPU上做SentencePiece分词，会阻塞GPU。

**优化方向：**
- 预分词：提前将数据转换为token ID，保存为二进制文件
- 异步加载：使用多线程预加载下一个batch
- GPU加速：将部分预处理移到GPU（如果支持）

### 问题2：显存使用率低（< 30%）

**可能原因：**
- Batch size太小
- 序列长度太短
- 模型参数少

**解决方案：**
```bash
# 增加batch size
transformer.exe --data ./data --batch-size 128

# 如果显存还有余量，可以增加模型大小
transformer.exe --data ./data --d-model 1024 --d-ff 4096
```

### 问题3：数据预处理时间过长

**症状：**
- 性能分析显示 `collate_fn` 占比 > 60%
- GPU利用率周期性下降（等待数据）

**解决方案：**

#### 临时方案：预分词数据
1. 创建预分词脚本，将JSON数据转换为token ID序列
2. 保存为二进制格式（.bin文件）
3. 训练时直接加载token ID，跳过分词步骤

#### 长期方案：异步数据加载
实现多线程数据加载器，在GPU计算时并行准备下一个batch。

### 问题4：同步操作过多

**症状：**
- GPU利用率波动大
- 每个batch之间有明显停顿

**解决方案：**
- 减少CPU-GPU同步操作
- 使用 `torch::NoGradGuard` 避免不必要的梯度计算
- 批量处理，减少小操作

## 三、性能优化检查清单

### 训练前检查
- [ ] 确认使用GPU：`--device 0` 或检查日志中的设备信息
- [ ] 检查GPU状态：运行 `nvidia-smi` 确认GPU可用
- [ ] 设置合适的batch size：从默认值开始，逐步增加

### 训练中监控
- [ ] 观察GPU利用率：应该 > 80%
- [ ] 观察显存使用：应该 > 50%
- [ ] 查看性能分析报告：识别瓶颈操作
- [ ] 监控训练速度：samples/s 应该稳定

### 优化调整
- [ ] 如果GPU利用率低：增加batch size
- [ ] 如果显存使用低：增加batch size或模型大小
- [ ] 如果数据预处理慢：考虑预分词
- [ ] 如果速度不提升：检查是否有其他瓶颈

## 四、性能基准参考

### 理想状态
- **GPU利用率**: 85-95%
- **显存使用率**: 70-90%
- **训练速度**: 持续稳定，无明显波动
- **数据预处理占比**: < 30%

### 警告状态
- **GPU利用率**: 50-80% → 需要优化
- **显存使用率**: 30-50% → batch size可以增加
- **数据预处理占比**: > 50% → 数据预处理是瓶颈

### 严重问题
- **GPU利用率**: < 50% → 严重使用不足
- **显存使用率**: < 20% → 资源浪费
- **训练速度波动大**: 可能存在同步问题

## 五、快速优化命令

```bash
# 1. 基础训练（默认配置）
transformer.exe --data ./data

# 2. 增加batch size（推荐第一步）
transformer.exe --data ./data --batch-size 64

# 3. 进一步增加batch size（如果显存充足）
transformer.exe --data ./data --batch-size 128

# 4. 如果batch size已经很大，可以增加模型大小
transformer.exe --data ./data --batch-size 128 --d-model 1024 --d-ff 4096

# 5. 监控GPU状态（另一个终端）
nvidia-smi -l 1
```

## 六、性能分析示例

### 正常情况
```
操作名称 | 总时间(ms) | 次数 | 平均(ms) | 占比(%)
-----------------------------------
collate_fn |    500.00 |     5 |  100.00 |  20.0
forward    |   1500.00 |     5 |  300.00 |  60.0
loss_compute|   500.00 |     5 |  100.00 |  20.0
```
**分析：** forward占比高，说明GPU在计算，这是正常的。

### 数据预处理瓶颈
```
操作名称 | 总时间(ms) | 次数 | 平均(ms) | 占比(%)
-----------------------------------
collate_fn |   2000.00 |     5 |  400.00 |  80.0
forward    |    300.00 |     5 |   60.00 |  12.0
loss_compute|   200.00 |     5 |   40.00 |   8.0
```
**分析：** collate_fn占比过高，GPU在等待数据，需要优化数据预处理。

### Batch Size太小
```
操作名称 | 总时间(ms) | 次数 | 平均(ms) | 占比(%)
-----------------------------------
collate_fn |    100.00 |    50 |    2.00 |  20.0
forward    |    200.00 |    50 |    4.00 |  40.0
loss_compute|   200.00 |    50 |    4.00 |  40.0
```
**分析：** 每个batch时间很短，batch数量多，说明batch size太小，GPU利用率低。

## 七、当前项目实测：batch=40 / 90 GPU 利用率分析

### 1. 实测现象

- **batch_size = 40** 时：GPU 利用率约 **20%**
- **batch_size = 90** 时：GPU 利用率约 **30%**

说明：即使增大 batch，GPU 利用率仍明显偏低（远低于理想的 80%+），说明 **GPU 计算并不是主要瓶颈**，而是被其他阶段拖慢了整体流水线。

### 2. 分析步骤（建议按顺序执行）

#### 步骤 1：查看一次 epoch 的性能分析输出

在第一个 epoch 结束后，程序会输出类似表格（伪示例）：

```text
========== GPU性能分析 ==========
操作名称       | 总时间(ms) | 次数 | 平均(ms) | 占比(%)
-----------------------------------------
collate_fn     |   30000.0  |   N  |  ...    |  55.0
forward        |   18000.0  |   N  |  ...    |  33.0
loss_compute   |    6000.0  |   N  |  ...    |  12.0
```

**重点观察：**

- `collate_fn` 占比是否仍然 **> 50%**：
  - 如果是，说明虽然已经做了批量分词、预取和 CUDA Streams，但 **CPU 侧的数据准备（SentencePiece 分词 + 构造 tensor + CPU→GPU 传输）仍然是主瓶颈**。
- `forward + loss_compute` 总占比是否明显 **< 50%**：
  - 如果 GPU 计算时间占比本身就不高，即使 batch 从 40 增加到 90，GPU 利用率也只能从 20% 提升到 30%左右。

#### 步骤 2：对比不同 batch_size 的时间结构

分别用 `batch_size=40` 和 `batch_size=90` 运行，记录：

- `collate_fn` 平均耗时 / 占比
- `forward` 平均耗时 / 占比
- `loss_compute` 平均耗时 / 占比

**典型情况：**

- 增大 batch_size 后，`forward` 时间会增加（一次算更多样本），但 `collate_fn` 也会增加（一次分词更多句子）
- 如果 `collate_fn` 增长幅度接近甚至超过 `forward` 的增长幅度，那么整体瓶颈依然在 CPU / 数据侧 → GPU 利用率只能缓慢提升

#### 步骤 3：检查数据预取与 CUDA Streams 是否生效

当前项目已经实现：

- **批量分词**：`SentencePieceTokenizer::encode_as_ids_batch`
- **数据预取**：`prefetch_mode` 支持 `none|async|thread`，默认 `async`
- **CUDA Streams**：传输 Stream + 计算 Stream 的流水线并行

建议：

1. 分别用三种模式测试：
   - `--prefetch none`
   - `--prefetch async`
   - `--prefetch thread`
2. 观察 `collate_fn` 占比和总 epoch 时间是否有明显变化：
   - 如果从 `none` → `async/thread` 总时间几乎不变，说明：
     - 数据准备与计算之间的重叠仍不足，或者
     - 绝大部分开销都发生在无法重叠的部分（如 SentencePiece 内部算法）

#### 步骤 4：评估模型规模与 GPU 计算负载

查看当前配置：

- `d_model`，`n_layers`，`n_heads`，`d_ff`
- 平均序列长度（可在 `collate_fn` 中打印 `max_src_len`, `max_tgt_len` 做一次统计）

如果：

- 模型维度中等 (`d_model=512`, `n_layers=6` 等)
- 序列长度不长（例如 20–40 token）

则单个 batch 的 **FLOPs（计算量）本身就不算高**，在这种情况下：

- 即使 batch 从 40 → 90，GPU 负载从「很低」变成「稍微重一点」，利用率从 20% → 30%，仍然是“轻载工作”

这解释了：**增大 batch size 只能有限地提升利用率，而不会一下子跳到 80%+。**

#### 步骤 5：结合 nvidia-smi 分析“波形”

用 `nvidia-smi -l 1` 观察训练过程：

- 如果 GPU 利用率呈现 **尖峰 + 掉到 0% 的锯齿型**，通常说明：
  - 计算阶段 GPU 全速（尖峰）
  - 数据准备阶段 GPU 空闲（掉到 0%）
- 如果利用率整体在 20–30% 小幅波动，而显存保持稳定：
  - 说明 GPU 始终在执行计算，但单次计算量较小（模型偏小 / batch 偏小）

针对你当前的现象（batch=40 → 20%，batch=90 → 30%），很可能是第二种情况与第一种混合：

- 模型规模中等，单次计算量有限
- 数据侧仍占有不小的比例，预取和 Streams 已经缓解但未完全消除

### 3. 结论：当前 GPU 利用率不高的主要原因

综合当前项目的实现和你的测试结果，可以得出一个相对明确的结论：

1. **CPU 侧数据准备仍是重要瓶颈**
   - 即使已经做了：批量 SentencePiece 分词 + 批量张量构建 + 异步 CPU→GPU 传输 + 数据预取 + CUDA Streams，
   - `collate_fn` 在性能分析中依然会占据较高比例（建议你实际看一下日志的具体数值）。

2. **模型规模与序列长度限制了单次 GPU 计算负载**
   - 当前 Transformer 配置更偏向“中等规模 + 通用训练”，不是超大模型；
   - 对于这样的模型和序列长度，单个 batch 的 FLOPs 不足以长时间“压满”高端 GPU 的算力。

3. **增加 batch size 有效，但收益逐渐递减**
   - 从 40 → 90，利用率从 20% → 30%，说明确实起作用；
   - 但想从 30% 推到 80% 以上，仅靠再加 batch 很可能会遇到显存上限或梯度稳定性问题，而且收益会越来越小。

4. **当前实现已经涵盖了常规工程级优化**
   - 批量分词、异步传输、预取、CUDA Streams、显存管理等都已加入；
   - 剩余的提升空间，更多依赖于 **任务本身的计算密度**（更大模型 / 更长序列 / 多卡并行）。

### 4. 后续可选的进一步优化方向（如有必要）

1. **预分词 + 二进制数据格式**
   - 在离线阶段将 JSON 文本全部转换为 token ID 序列，保存为 `.bin` 或自定义格式；
   - 训练时不再调用 SentencePiece，仅做简单的 `memcpy` + `view` 操作。

2. **进一步放大模型或序列长度（如果业务允许）**
   - 增大 `d_model`、`d_ff`、`n_layers`；
   - 训练时使用更长的 `max_len` 或更长句子的数据集；
   - 这会显著增加每个 batch 的计算量，从而提高 GPU 利用率。

3. **多卡并行 / 分布式训练**
   - 单卡利用率 30% 时，多卡并行可以“横向扩展”，提升整体吞吐量；
   - 需要额外引入分布式通信（NCCL 等），工程成本更高。

4. **更激进的流水线并行**
   - 进一步拆分前向 / 反向 / 梯度同步等阶段，做更细粒度的 Stream 排布；
   - 一般只在超大模型、多 GPU 场景下才值得投入。


## 八、总结

1. **首先增加batch size**：这是最简单有效的优化方法，但收益会逐渐递减
2. **监控GPU利用率**：使用 `nvidia-smi` 实时监控整体趋势和波形
3. **查看性能分析**：依靠程序内的 `collate_fn/forward/loss_compute` 统计识别瓶颈
4. **结合模型规模与数据特征**：判断是“GPU太闲”还是“模型本身计算量就不大”
5. **逐步优化**：先解决工程侧瓶颈（数据预处理、预取、Streams），再考虑扩模/多卡等大改动

如果按照以上方法优化后GPU利用率仍然 < 50%，可能需要：
- 检查数据预处理是否在GPU上
- 考虑使用更大的模型
- 检查是否有其他进程占用GPU

