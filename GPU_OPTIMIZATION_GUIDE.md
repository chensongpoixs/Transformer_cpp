# GPU性能优化指南

## 一、排查GPU使用不足的方法

### 1. 使用内置性能分析工具

训练程序会在第一个epoch结束后自动输出性能分析报告：

```
========== GPU性能分析 ==========
操作名称 | 总时间(ms) | 次数 | 平均(ms) | 最小(ms) | 最大(ms) | 占比(%)
-----------------------------------
collate_fn          |    1500.00 |     5 |  300.00 |  280.00 |  320.00 |  60.0
forward             |     800.00 |     5 |  160.00 |  150.00 |  170.00 |  32.0
loss_compute        |     200.00 |     5 |   40.00 |   35.00 |   45.00 |   8.0
```

**分析要点：**
- 如果 `collate_fn` 占比 > 50%，说明数据预处理是瓶颈
- 如果 `forward` 占比 < 20%，说明GPU计算时间短，可能batch太小
- 如果各操作时间差异大，说明存在同步等待

### 2. 使用 nvidia-smi 监控

在另一个终端运行：
```bash
# Windows PowerShell
nvidia-smi -l 1

# Linux
watch -n 1 nvidia-smi
```

**关键指标：**
- **GPU利用率 (Util%)**：应该 > 80%，如果 < 50% 说明使用不足
- **显存使用 (Memory-Usage)**：应该 > 50%，如果 < 20% 说明batch太小
- **功耗 (Power)**：应该接近TDP，如果很低说明GPU空闲

### 3. 检查程序输出的GPU状态

训练开始时会自动输出：
```
========== GPU状态检查 ==========
GPU数量: 1
当前使用GPU: 0
GPU名称: NVIDIA GeForce RTX 3090
显存大小: 24.00 GB
已分配显存: 2048.00 MB
缓存显存: 512.00 MB
总使用显存: 2560.00 MB
```

## 二、常见问题及解决方案

### 问题1：GPU利用率低（< 50%）

**可能原因：**
1. Batch size太小
2. 数据预处理在CPU上阻塞GPU
3. 模型太小，无法充分利用GPU

**解决方案：**

#### 方案1：增加Batch Size
```bash
# 逐步增加batch size，直到显存接近满载
transformer.exe --data ./data --batch-size 64
transformer.exe --data ./data --batch-size 128
transformer.exe --data ./data --batch-size 256
```

**判断标准：**
- 显存使用率 > 80%
- GPU利用率 > 80%
- 训练速度提升

#### 方案2：优化数据预处理（未来实现）
当前 `collate_fn` 在CPU上做SentencePiece分词，会阻塞GPU。

**优化方向：**
- 预分词：提前将数据转换为token ID，保存为二进制文件
- 异步加载：使用多线程预加载下一个batch
- GPU加速：将部分预处理移到GPU（如果支持）

### 问题2：显存使用率低（< 30%）

**可能原因：**
- Batch size太小
- 序列长度太短
- 模型参数少

**解决方案：**
```bash
# 增加batch size
transformer.exe --data ./data --batch-size 128

# 如果显存还有余量，可以增加模型大小
transformer.exe --data ./data --d-model 1024 --d-ff 4096
```

### 问题3：数据预处理时间过长

**症状：**
- 性能分析显示 `collate_fn` 占比 > 60%
- GPU利用率周期性下降（等待数据）

**解决方案：**

#### 临时方案：预分词数据
1. 创建预分词脚本，将JSON数据转换为token ID序列
2. 保存为二进制格式（.bin文件）
3. 训练时直接加载token ID，跳过分词步骤

#### 长期方案：异步数据加载
实现多线程数据加载器，在GPU计算时并行准备下一个batch。

### 问题4：同步操作过多

**症状：**
- GPU利用率波动大
- 每个batch之间有明显停顿

**解决方案：**
- 减少CPU-GPU同步操作
- 使用 `torch::NoGradGuard` 避免不必要的梯度计算
- 批量处理，减少小操作

## 三、性能优化检查清单

### 训练前检查
- [ ] 确认使用GPU：`--device 0` 或检查日志中的设备信息
- [ ] 检查GPU状态：运行 `nvidia-smi` 确认GPU可用
- [ ] 设置合适的batch size：从默认值开始，逐步增加

### 训练中监控
- [ ] 观察GPU利用率：应该 > 80%
- [ ] 观察显存使用：应该 > 50%
- [ ] 查看性能分析报告：识别瓶颈操作
- [ ] 监控训练速度：samples/s 应该稳定

### 优化调整
- [ ] 如果GPU利用率低：增加batch size
- [ ] 如果显存使用低：增加batch size或模型大小
- [ ] 如果数据预处理慢：考虑预分词
- [ ] 如果速度不提升：检查是否有其他瓶颈

## 四、性能基准参考

### 理想状态
- **GPU利用率**: 85-95%
- **显存使用率**: 70-90%
- **训练速度**: 持续稳定，无明显波动
- **数据预处理占比**: < 30%

### 警告状态
- **GPU利用率**: 50-80% → 需要优化
- **显存使用率**: 30-50% → batch size可以增加
- **数据预处理占比**: > 50% → 数据预处理是瓶颈

### 严重问题
- **GPU利用率**: < 50% → 严重使用不足
- **显存使用率**: < 20% → 资源浪费
- **训练速度波动大**: 可能存在同步问题

## 五、快速优化命令

```bash
# 1. 基础训练（默认配置）
transformer.exe --data ./data

# 2. 增加batch size（推荐第一步）
transformer.exe --data ./data --batch-size 64

# 3. 进一步增加batch size（如果显存充足）
transformer.exe --data ./data --batch-size 128

# 4. 如果batch size已经很大，可以增加模型大小
transformer.exe --data ./data --batch-size 128 --d-model 1024 --d-ff 4096

# 5. 监控GPU状态（另一个终端）
nvidia-smi -l 1
```

## 六、性能分析示例

### 正常情况
```
操作名称 | 总时间(ms) | 次数 | 平均(ms) | 占比(%)
-----------------------------------
collate_fn |    500.00 |     5 |  100.00 |  20.0
forward    |   1500.00 |     5 |  300.00 |  60.0
loss_compute|   500.00 |     5 |  100.00 |  20.0
```
**分析：** forward占比高，说明GPU在计算，这是正常的。

### 数据预处理瓶颈
```
操作名称 | 总时间(ms) | 次数 | 平均(ms) | 占比(%)
-----------------------------------
collate_fn |   2000.00 |     5 |  400.00 |  80.0
forward    |    300.00 |     5 |   60.00 |  12.0
loss_compute|   200.00 |     5 |   40.00 |   8.0
```
**分析：** collate_fn占比过高，GPU在等待数据，需要优化数据预处理。

### Batch Size太小
```
操作名称 | 总时间(ms) | 次数 | 平均(ms) | 占比(%)
-----------------------------------
collate_fn |    100.00 |    50 |    2.00 |  20.0
forward    |    200.00 |    50 |    4.00 |  40.0
loss_compute|   200.00 |    50 |    4.00 |  40.0
```
**分析：** 每个batch时间很短，batch数量多，说明batch size太小，GPU利用率低。

## 七、总结

1. **首先增加batch size**：这是最简单有效的优化方法
2. **监控GPU利用率**：使用 `nvidia-smi` 实时监控
3. **查看性能分析**：程序会自动输出，识别瓶颈
4. **逐步优化**：根据分析结果针对性优化

如果按照以上方法优化后GPU利用率仍然 < 50%，可能需要：
- 检查数据预处理是否在GPU上
- 考虑使用更大的模型
- 检查是否有其他进程占用GPU

